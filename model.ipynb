{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup|\n",
    "!pip install transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from datasets import load_metric\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    DataCollatorWithPadding, \n",
    "    EarlyStoppingCallback,\n",
    "    AdamW,\n",
    "    get_cosine_schedule_with_warmup\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd drive/MyDrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출처: https://dacon.io/en/competitions/official/235875/codeshare/4441?page=1&dtype=recent\n",
    "def get_basic_example_fn(tokenizer, src_cols=[], tar_col='label', label_fn=None, max_src_len=256, max_tar_len=256, truncation=True, padding=\"max_length\"):\n",
    "\n",
    "    def example_fn(examples):\n",
    "        output = tokenizer(*[examples[col] for col in [c for c in src_cols if c in examples]],\n",
    "                           padding=padding,\n",
    "                           max_length=max_src_len,\n",
    "                           truncation=True)\n",
    "        if tar_col in examples:\n",
    "            output[\"labels\"] = [label_fn(c) for c in examples[tar_col]] if label_fn else examples[tar_col]\n",
    "        return output\n",
    "    \n",
    "    return example_fn\n",
    "\n",
    "metric_fn = load_metric('glue', 'mnli')\n",
    "\n",
    "def metric(p):\n",
    "    preds, labels = p\n",
    "    if not isinstance(preds, tuple) and not isinstance(preds, list):\n",
    "        if len(preds.shape) == 2 and preds.shape[1] == 1:\n",
    "            preds = preds[:, 0]\n",
    "        elif len(preds.shape) - len(labels.shape) == 1:\n",
    "            preds = np.argmax(preds, axis=-1)\n",
    "    return metric_fn.compute(predictions=preds, references=labels)\n",
    "\n",
    "\n",
    "# back-translation function for data augmentation.\n",
    "def backtranslate_text(text):\n",
    "    import six\n",
    "    from google.cloud import translate_v2 as translate\n",
    "\n",
    "    translate_client = translate.Client()\n",
    "    if isinstance(text, six.binary_type):\n",
    "        text = text.decode(\"utf-8\")\n",
    "\n",
    "    eng = translate_client.translate(text, target_language=\"en\")\n",
    "    eng = [t[\"translatedText\"] for t in eng]\n",
    "    kor = translate_client.translate(eng, target_language=\"ko\")\n",
    "    kor = [t[\"translatedText\"] for t in kor]\n",
    "    print(\"{} - > {}\".format(text, kor))\n",
    "\n",
    "    return kor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back translation examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *_gen이 backtranslation을 통해 생성된 문장입니다.\n",
    "aug_df = pd.read_csv(\"dacon/knlu/data/klue-xnli-hypo_premise_gen_include.csv\")\n",
    "print(aug_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = \"dacon/knlu/data/klue_xnli_aug_multi_snli.csv\"\n",
    "# EVAL = 'dacon/knlu/data/full_test_data.csv'\n",
    "TEST = \"dacon/knlu/data/test_data.csv\"\n",
    "CKPT = \"dacon/knlu/runs/\"\n",
    "# MODEL = 'klue/roberta-large' # 1st train\n",
    "MODEL = 'klue/roberta-large-full-data-trained' # 2nd train\n",
    "\n",
    "MAX_LEN = 256\n",
    "TRAIN_BATCH = 8\n",
    "EVAL_BATCH = 16\n",
    "\n",
    "map_dict = {'contradiction': 0, 'neutral': 1, 'entailment': 2}\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "example_fn = get_basic_example_fn(\n",
    "    tokenizer,\n",
    "    src_cols=['premise', 'hypothesis'], \n",
    "    tar_col='label', \n",
    "    label_fn=lambda x: map_dict.get(x),\n",
    "    max_src_len=MAX_LEN,\n",
    "    max_tar_len=MAX_LEN,\n",
    "    truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\":TRAIN})\n",
    "dataset = dataset['train'].map(example_fn, remove_columns=['index', 'premise', 'hypothesis', 'label'], batched=True)\n",
    "dataset = dataset.train_test_split(0.03)\n",
    "train_data, eval_data = dataset['train'], dataset['test']\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=len(map_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer-wise learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "learning_rate = 2e-5\n",
    "mu = 0.95\n",
    "\n",
    "group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n",
    "group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n",
    "group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n",
    "group4=['layer.12.','layer.13.','layer.14.','layer.15.']\n",
    "group5=['layer.16.','layer.17.','layer.18.','layer.19.']\n",
    "group6=['layer.20.','layer.21.','layer.22.','layer.23.']\n",
    "groupall = ['layer.0.','layer.1.','layer.2.','layer.3.', 'layer.4.','layer.5.','layer.6.','layer.7.', 'layer.8.','layer.9.','layer.10.','layer.11.', 'layer.12.','layer.13.','layer.14.','layer.15.', 'layer.16.','layer.17.','layer.18.','layer.19.', 'layer.20.','layer.21.','layer.22.','layer.23.']\n",
    "\n",
    "optimizer_parameters = [{'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in group1) or not any(nd in n for nd in groupall)], 'lr':learning_rate*mu*mu*mu*mu*mu},\n",
    "                        {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in group2)], 'lr':learning_rate*mu*mu*mu*mu},\n",
    "                        {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in group3)], 'lr':learning_rate*mu*mu*mu},\n",
    "                        {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in group4)], 'lr':learning_rate*mu*mu},\n",
    "                        {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in group5)], 'lr':learning_rate*mu},\n",
    "                        {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in group6)], 'lr':learning_rate},\n",
    "                        {'params': [p for n, p in model.classifier.named_parameters()], 'lr':learning_rate}]\n",
    "\n",
    "optimizer = AdamW(optimizer_parameters, lr=learning_rate, weight_decay=0.01, correct_bias=False)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=230, num_training_steps=2290)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"runs/roberta_large_hypopreaug_layerlr/\",\n",
    "    per_device_train_batch_size=TRAIN_BATCH,\n",
    "    per_device_eval_batch_size=EVAL_BATCH,\n",
    "    logging_strategy='steps',\n",
    "    evaluation_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    save_steps=230,\n",
    "    save_total_limit=3,\n",
    "    metric_for_best_model='accuracy',\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    label_smoothing_factor=0.025,\n",
    "    gradient_accumulation_steps=128,\n",
    "    logging_steps=230,\n",
    "    eval_steps=230,\n",
    "    num_train_epochs=5\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    compute_metrics=metric,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "    optimizers=(optimizer, lr_scheduler)\n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Training.\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "model = AutoModelForSequenceClassification.from_pretrained('runs/roberta_large_hypopreaug_layerlr/checkpoint-690')\n",
    "\n",
    "# arguments for Trainer\n",
    "test_args = TrainingArguments(\n",
    "    output_dir = \"runs/roberta_large_fdtrain_hypopreaug/\",\n",
    "    do_train = False,\n",
    "    do_predict = True,\n",
    "    per_device_eval_batch_size = EVAL_BATCH,   \n",
    "    dataloader_drop_last = False    \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=test_args,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    compute_metrics=metric,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "# Prepare the test data.\n",
    "test_data = load_dataset(\"csv\", data_files={\"test\":TEST})\n",
    "test_data = test_data['test'].remove_columns(\"label\").map(example_fn, batched=True, remove_columns = ['premise', 'hypothesis'])\n",
    "\n",
    "# Predict the test outputs.\n",
    "outputs = trainer.predict(test_data)\n",
    "\n",
    "# Transform the test outputs for the submission.\n",
    "df_sub = pd.DataFrame({\"index\":test_data['index'], \"label\":np.argmax(outputs.predictions, axis=-1)})\n",
    "df_sub['label'] = df_sub['label'].apply(lambda x: {v:k for k,v in map_dict.items()}[x])\n",
    "df_sub.to_csv(\"submission-roberta-large-final.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
